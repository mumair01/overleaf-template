@inproceedings{wesseling2006sufficiency,
  title        = {On the sufficiency and redundancy of pitch for TRP projection.},
  author       = {Wesseling, Wieneke and van Son, Rob and Pols, Louis CW and others},
  year         = 2006,
  booktitle    = {Interspeech},
  organization = {Citeseer}
}
@article{levinson2016turn,
  title     = {Turn-taking in human communication--origins and implications for language processing},
  author    = {Levinson, Stephen C},
  year      = 2016,
  journal   = {Trends in cognitive sciences},
  publisher = {Elsevier},
  volume    = 20,
  number    = 1,
  pages     = {6--14}
}
@article{riest2015,
  title     = {Anticipation in turn-taking: mechanisms and information sources},
  author    = {Riest, Carina and Jorschick, Annett B and De Ruiter, Jan-Peter},
  year      = 2015,
  journal   = {Frontiers in psychology},
  publisher = {Frontiers},
  volume    = 6,
  pages     = 89
}
@article{magyari2022predictions,
  title     = {Predictions in conversation},
  author    = {Magyari, Lilla},
  year      = 2022,
  journal   = {A Life in Cognition: Studies in Cognitive Science in Honor of Csaba Pl{\'e}h},
  publisher = {Springer},
  pages     = {59--75}
}
@article{magyari2014early,
  title     = {Early anticipation lies behind the speed of response in conversation},
  author    = {Magyari, Lilla and Bastiaansen, Marcel CM and De Ruiter, Jan P and Levinson, Stephen C},
  year      = 2014,
  journal   = {Journal of Cognitive Neuroscience},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  volume    = 26,
  number    = 11,
  pages     = {2530--2539}
}
@article{magyari2017temporal,
  title     = {Temporal preparation for speaking in question-answer sequences},
  author    = {Magyari, Lilla and De Ruiter, Jan P and Levinson, Stephen C},
  year      = 2017,
  journal   = {Frontiers in Psychology},
  publisher = {Frontiers},
  volume    = 8,
  pages     = 230845
}
@inproceedings{magyari2008timing,
  title        = {Timing in conversation: the anticipation of turn endings},
  author       = {Magyari, Lilla and De Ruiter, Jan Peter},
  year         = 2008,
  booktitle    = {12th Workshop on the Semantics and Pragmatics Dialogue},
  pages        = {139--146},
  organization = {King's college}
}
@article{magyari2012prediction,
  title     = {Prediction of turn-ends based on anticipation of upcoming words},
  author    = {Magyari, Lilla and De Ruiter, Jan P},
  year      = 2012,
  journal   = {Frontiers in psychology},
  publisher = {Frontiers Media SA},
  volume    = 3,
  pages     = 376
}
@article{kreps2022all,
  title     = {All the news that’s fit to fabricate: AI-generated text as a tool of media misinformation},
  author    = {Kreps, Sarah and McCain, R Miles and Brundage, Miles},
  year      = 2022,
  journal   = {Journal of Experimental Political Science},
  publisher = {Cambridge University Press},
  volume    = 9,
  number    = 1,
  pages     = {104--117}
}
@article{john1993switchboard,
  title   = {Switchboard-1 Release 2 LDC97S62},
  author  = {John, Godfrey and Holliman, Edward},
  year    = 1993,
  journal = {Web Download. Philadelphia: Linguistic Data Consortium},
  pages   = 35
}
@inproceedings{dingemanse2022text,
  title     = {From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology},
  author    = {Dingemanse, Mark and Liesenfeld, Andreas},
  year      = 2022,
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {5614--5633}
}
Two main findings of this empirical study are that (1) conversational feedback is markedly less frequent in subtitles than in spontaneous dialogues and (2) subtitles contain a higher proportion of negative feedback.
@article{pilan2023conversational,
  title   = {Conversational feedback in scripted versus spontaneous dialogues: A comparative analysis},
  author  = {Pil{\'a}n, Ildik{\'o} and Pr{\'e}vot, Laurent and Buschmeier, Hendrik and Lison, Pierre},
  year    = 2023,
  journal = {arXiv preprint arXiv:2309.15656}
}
@article{schegloff1968sequencing,
  title     = {Sequencing in conversational openings 1},
  author    = {Schegloff, Emanuel A},
  year      = 1968,
  journal   = {American anthropologist},
  publisher = {Wiley Online Library},
  volume    = 70,
  number    = 6,
  pages     = {1075--1095}
}
@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year    = 2019,
  journal = {OpenAI blog},
  volume  = 1,
  number  = 8,
  pages   = 9
}
@incollection{grice1975logic,
  title     = {Logic and conversation},
  author    = {Grice, Herbert P},
  year      = 1975,
  booktitle = {Speech acts},
  publisher = {Brill},
  pages     = {41--58}
}
@phdthesis{warnke2022speech,
  title  = {The Communication of Social Meaning in Conversation},
  author = {Warnke, Lena},
  year   = 2024,
  school = {Tufts University}
}
@article{warnke2023top,
  title    = {Top-down effect of dialogue coherence on perceived speaker identity},
  author   = {Warnke, Lena and de Ruiter, Jan P.},
  year     = 2023,
  month    = {Mar},
  day      = {01},
  journal  = {Scientific Reports},
  volume   = 13,
  number   = 1,
  pages    = 3458,
  doi      = {10.1038/s41598-023-30435-z},
  issn     = {2045-2322},
  url      = {https://doi.org/10.1038/s41598-023-30435-z},
  abstract = {A key mechanism in the comprehension of conversation is the ability for listeners to recognize who is speaking and when a speaker switch occurs. Some authors suggest that speaker change detection is accomplished through bottom-up mechanisms in which listeners draw on changes in the acoustic features of the auditory signal. Other accounts propose that speaker change detection involves drawing on top-down linguistic representations to identify who is speaking. The present study investigates these hypotheses experimentally by manipulating the pragmatic coherence of conversational utterances. In experiment 1, participants listened to pairs of utterances and had to indicate whether they heard the same or different speakers. Even though all utterances were spoken by the same speaker, our results show that when two segments of conversation are spoken by the same speaker but make sense for different speakers to say, listeners report hearing different speakers. In experiment 2 we removed pragmatic information from the same stimuli by scrambling word order while leaving acoustic information intact. In contrast to experiment 1, results from the second experiment indicate no difference between our experimental conditions. We interpret these results as a top-down effect of pragmatic expectations: knowledge of conversational structure at least partially determines a listener's perception of speaker changes in conversation.}
}
@article{brothers2021word,
  title     = {Word predictability effects are linear, not logarithmic: Implications for probabilistic models of sentence comprehension},
  author    = {Brothers, Trevor and Kuperberg, Gina R},
  year      = 2021,
  journal   = {Journal of Memory and Language},
  publisher = {Elsevier},
  volume    = 116,
  pages     = 104174
}
@inproceedings{hale2001probabilistic,
  title     = {A Probabilistic Earley Parser as a Psycholinguistic Model},
  author    = {Hale, John},
  year      = 2001,
  booktitle = {Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies},
  location  = {Pittsburgh, Pennsylvania},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  series    = {NAACL '01},
  pages     = {1–8},
  doi       = {10.3115/1073336.1073357},
  url       = {https://doi.org/10.3115/1073336.1073357},
  abstract  = {In human sentence processing, cognitive load can be defined many ways. This report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at some point in a sentence: the surprisal of word wi given its prefix wo...i-1 on a phrase-structural language model. These loads can be efficiently calculated using a probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time on a word-by-word basis. Under grammatical assumptions supported by corpus-frequency data, the operation of Stolcke's probabilistic Earley parser correctly predicts processing phenomena associated with garden path structural ambiguity and with the subject/object relative asymmetry.},
  numpages  = 8
}
@article{oh2022comparison,
  title     = {Comparison of structural parsers and neural language models as surprisal estimators},
  author    = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year      = 2022,
  journal   = {Frontiers in Artificial Intelligence},
  publisher = {Frontiers Media SA},
  volume    = 5
}
@article{kutas1980event,
  title     = {Event-related brain potentials to semantically inappropriate and surprisingly large words},
  author    = {Kutas, Marta and Hillyard, Steven A},
  year      = 1980,
  journal   = {Biological psychology},
  publisher = {Elsevier},
  volume    = 11,
  number    = 2,
  pages     = {99--116}
}
@article{mahowald2013info,
  title     = {Info/information theory: Speakers choose shorter words in predictive contexts},
  author    = {Mahowald, Kyle and Fedorenko, Evelina and Piantadosi, Steven T and Gibson, Edward},
  year      = 2013,
  journal   = {Cognition},
  publisher = {Elsevier},
  volume    = 126,
  number    = 2,
  pages     = {313--318}
}
@inproceedings{frank2008speaking,
  title     = {Speaking rationally: Uniform information density as an optimal strategy for language production},
  author    = {Frank, Austin F and Jaeger, T Florain},
  year      = 2008,
  booktitle = {Proceedings of the annual meeting of the cognitive science society},
  volume    = 30,
  number    = 30
}
@article{xie2020uncertainty,
  title   = {Uncertainty and surprisal jointly deliver the punchline: Exploiting incongruity-based features for humor recognition},
  author  = {Xie, Yubo and Li, Junze and Pu, Pearl},
  year    = 2020,
  journal = {arXiv preprint arXiv:2012.12007}
}
@article{jaeger2006speakers,
  title   = {Speakers optimize information density through syntactic reduction},
  author  = {Jaeger, T and Levy, Roger},
  year    = 2006,
  journal = {Advances in neural information processing systems},
  volume  = 19
}
@article{kuribayashi2021lower,
  title   = {Lower perplexity is not always human-like},
  author  = {Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  year    = 2021,
  journal = {arXiv preprint arXiv:2106.01229}
}
@article{oh2022does,
  title     = {Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?},
  author    = {Oh, Byung-Doh and Schuler, William},
  year      = 2023,
  journal   = {Transactions of the Association for Computational Linguistics},
  publisher = {MIT Press},
  volume    = 11,
  pages     = {336--350}
}
@incollection{schegloff1979relevance,
  title     = {The relevance of repair to syntax-for-conversation},
  author    = {Schegloff, Emanuel A},
  year      = 1979,
  booktitle = {Discourse and syntax},
  publisher = {Brill},
  pages     = {261--286}
}
@inproceedings{wilcox2020predictive,
  title     = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  author    = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger P.},
  year      = 2020,
  booktitle = {Proceedings of the 42nd Annual Meeting of the Cognitive Science Society},
  pages     = {1707–1713}
}
@article{nguyen2023generative,
  title     = {Generative spoken dialogue language modeling},
  author    = {Nguyen, Tu Anh and Kharitonov, Eugene and Copet, Jade and Adi, Yossi and Hsu, Wei-Ning and Elkahky, Ali and Tomasello, Paden and Algayres, Robin and Sagot, Benoit and Mohamed, Abdelrahman and others},
  year      = 2023,
  journal   = {Transactions of the Association for Computational Linguistics},
  publisher = {MIT Press},
  volume    = 11,
  pages     = {250--266}
}
@inproceedings{zhang2022paradox,
  title     = {On the Paradox of Learning to Reason from Data},
  author    = {Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and Van den Broeck, Guy},
  year      = 2023,
  month     = {aug},
  booktitle = {Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI)},
  url       = {http://starai.cs.ucla.edu/papers/ZhangIJCAI23.pdf},
  code      = {https://github.com/joshuacnf/paradox-learning2reason},
  keywords  = {conference,selective}
}
@article{nguyen2022generative,
  title   = {Generative spoken dialogue language modeling},
  author  = {Nguyen, Tu Anh and Kharitonov, Eugene and Copet, Jade and Adi, Yossi and Hsu, Wei-Ning and Elkahky, Ali and Tomasello, Paden and Algayres, Robin and Sagot, Benoit and Mohamed, Abdelrahman and others},
  year    = 2022,
  journal = {arXiv preprint arXiv:2203.16502}
}
@article{baayen2008mixed,
  title     = {Mixed-effects modeling with crossed random effects for subjects and items},
  author    = {Baayen, R Harald and Davidson, Douglas J and Bates, Douglas M},
  year      = 2008,
  journal   = {Journal of memory and language},
  publisher = {Elsevier},
  volume    = 59,
  number    = 4,
  pages     = {390--412}
}
@article{ferreira2002good,
  title     = {Good-enough representations in language comprehension},
  author    = {Ferreira, Fernanda and Bailey, Karl GD and Ferraro, Vittoria},
  year      = 2002,
  journal   = {Current directions in psychological science},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  volume    = 11,
  number    = 1,
  pages     = {11--15}
}
@article{caucheteux2021long,
  title   = {Long-range and hierarchical language predictions in brains and algorithms},
  author  = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
  year    = 2021,
  journal = {arXiv preprint arXiv:2111.14232}
}
@inproceedings{caucheteux2021disentangling,
  title        = {Disentangling syntax and semantics in the brain with deep networks},
  author       = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
  year         = 2021,
  booktitle    = {International Conference on Machine Learning},
  pages        = {1336--1348},
  organization = {PMLR}
}
@article{caucheteux2022brains,
  title     = {Brains and algorithms partially converge in natural language processing},
  author    = {Caucheteux, Charlotte and King, Jean-R{\'e}mi},
  year      = 2022,
  journal   = {Communications biology},
  publisher = {Nature Publishing Group UK London},
  volume    = 5,
  number    = 1,
  pages     = 134
}
@book{jeffreys1939,
  title     = {Theory of Probability},
  author    = {Harold Jeffreys},
  year      = 1939,
  publisher = {Oxford, England: Clarendon Press},
  editor    = {}
}
@inproceedings{ekstedt2020turngpt,
  title     = {{T}urn{GPT}: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog},
  author    = {Ekstedt, Erik  and Skantze, Gabriel},
  year      = 2020,
  month     = nov,
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {2981--2990},
  doi       = {10.18653/v1/2020.findings-emnlp.268},
  url       = {https://aclanthology.org/2020.findings-emnlp.268},
  editor    = {Cohn, Trevor  and He, Yulan  and Liu, Yang},
  abstract  = {Syntactic and pragmatic completeness is known to be important for turn-taking prediction, but so far machine learning models of turn-taking have used such linguistic information in a limited way. In this paper, we introduce TurnGPT, a transformer-based language model for predicting turn-shifts in spoken dialog. The model has been trained and evaluated on a variety of written and spoken dialog datasets. We show that the model outperforms two baselines used in prior work. We also report on an ablation study, as well as attention and gradient analyses, which show that the model is able to utilize the dialog context and pragmatic completeness for turn-taking prediction. Finally, we explore the model{'}s potential in not only detecting, but also projecting, turn-completions.}
}
@inproceedings{cohn2019large,
  title     = {A large-scale user study of an Alexa prize chatbot: Effect of TTS dynamism on perceived quality of social dialog},
  author    = {Cohn, Michelle and Chen, Chun-Yen and Yu, Zhou},
  year      = 2019,
  booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
  pages     = {293--306}
}
@inproceedings{vaswani2017attention,
  title     = {Attention is All you Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  year      = 2017,
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  volume    = 30,
  pages     = {},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett}
}
@article{kieuvongngam2020automatic,
  title   = {Automatic text summarization of covid-19 medical research articles using bert and gpt-2},
  author  = {Kieuvongngam, Virapat and Tan, Bowen and Niu, Yiming},
  year    = 2020,
  journal = {arXiv preprint arXiv:2006.01997}
}
@article{kieuvongngam2020automatic,
  title   = {Automatic text summarization of covid-19 medical research articles using bert and gpt-2},
  author  = {Kieuvongngam, Virapat and Tan, Bowen and Niu, Yiming},
  year    = 2020,
  journal = {arXiv preprint arXiv:2006.01997}
}
@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  year    = 2020,
  journal = {Advances in neural information processing systems},
  volume  = 33,
  pages   = {1877--1901}
}
@article{wolf2019transfertransfo,
  title   = {Transfertransfo: A transfer learning approach for neural network based conversational agents},
  author  = {Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  year    = 2019,
  journal = {arXiv preprint arXiv:1901.08149}
}
@article{umair2022gailbot,
  title   = {GailBot: An automatic transcription system for Conversation Analysis},
  author  = {Umair, Muhammad and Mertens, Julia Beret and Albert, Saul and De Ruiter, Jan-Peter},
  year    = 2022,
  journal = {Dialogue \& Discourse},
  volume  = 13,
  number  = 1,
  pages   = {63--95}
}
@article{chatgptfunholden23,
  title    = {ChatGPT is fun, but not an author},
  author   = {H. Holden Thorp},
  year     = 2023,
  journal  = {Science},
  volume   = 379,
  number   = 6630,
  pages    = {313--313},
  doi      = {10.1126/science.adg7879},
  url      = {https://www.science.org/doi/abs/10.1126/science.adg7879},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.adg7879},
  abstract = {In less than 2 months, the artificial intelligence (AI) program ChatGPT has become a cultural sensation. It is freely accessible through a web portal created by the tool’s developer, OpenAI. The program—which automatically creates text based on written prompts—is so popular that it’s likely to be “at capacity right now” if you attempt to use it. When you do get through, ChatGPT provides endless entertainment. I asked it to rewrite the first scene of the classic American play Death of a Salesman, but to feature Princess Elsa from the animated movie Frozen as the main character instead of Willy Loman. The output was an amusing conversation in which Elsa—who has come home from a tough day of selling—is told by her son Happy, “Come on, Mom. You’re Elsa from Frozen. You have ice powers and you’re a queen. You’re unstoppable.” Mash-ups like this are certainly fun, but there are serious implications for generative AI programs like ChatGPT in science and academia.}
}
@article{kung2023performance,
  title     = {Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models},
  author    = {Kung, Tiffany H and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepa{\~n}o, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and others},
  year      = 2023,
  journal   = {PLoS digital health},
  publisher = {Public Library of Science},
  volume    = 2,
  number    = 2,
  pages     = {e0000198}
}
@article{avila2023chatgpt,
  title   = {Chatgpt as a Support Tool for Online Behavioral Task Programming},
  author  = {Avila-Chauvet, Laurent and Mej{\'\i}a, Diana and Acosta Quiroz, Christian Oswaldo},
  year    = 2023,
  journal = {Available at SSRN 4329020},
  url     = {https://ssrn.com/abstract=4329020}
}
@article{dale_2021,
  title     = {GPT-3: What’s it good for?},
  author    = {Dale, Robert},
  year      = 2021,
  journal   = {Natural Language Engineering},
  publisher = {Cambridge University Press},
  volume    = 27,
  number    = 1,
  pages     = {113--118}
}
@article{chatgptgooglemicrosoft_heaven2_2023,
  title   = {The ChatGPT-fueled battle for search is bigger than Microsoft or Google},
  author  = {Will Douglas Heaven},
  year    = 2023,
  journal = {MIT Technology Review},
  url     = {https://www.technologyreview.com/2023/02/16/1068695/chatgpt-chatbot-battle-search-microsoft-bing-google/}
}
@article{chatgptmicrosoft_melissa_2023,
  title   = {Here’s how Microsoft could use ChatGPT},
  author  = {Melissa Heikkilä},
  year    = 2023,
  journal = {MIT Technology Review},
  url     = {https://www.technologyreview.com/2023/01/17/1067014/heres-how-microsoft-could-use-chatgpt/}
}
@article{wiredchatgpt_2023,
  title   = {Review: We Put ChatGPT, Bing Chat, and Bard to the Test},
  author  = {Lauren Godge},
  year    = 2023,
  journal = {WIRED},
  url     = {https://www.wired.com/story/review-ai-chatbots-bing-bard-chat-gpt/}
}
@article{mahowald2023dissociating,
  title   = {Dissociating language and thought in large language models: a cognitive perspective},
  author  = {Mahowald, Kyle and Ivanova, Anna A and Blank, Idan A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  year    = 2023,
  journal = {arXiv preprint arXiv:2301.06627}
}
@article{liu2021gpt,
  title   = {GPT understands, too},
  author  = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  year    = 2021,
  journal = {arXiv preprint arXiv:2103.10385}
}
@article{caucheteux2021gpt,
  title     = {GPT-2’s activations predict the degree of semantic comprehension in the human brain},
  author    = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-R{\'e}mi},
  year      = 2021,
  journal   = {BioRxiv},
  publisher = {Cold Spring Harbor Laboratory},
  pages     = {2021--04}
}

@techreport{srivastava2022beyond,
  title  = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author = {Aitor Lewkowycz and Ambrose Slone and Anders Andreassen and Daniel Freeman and Ethan S Dyer and Gaurav Mishra and Guy Gur-Ari and Jaehoon Lee and Jascha Sohl-dickstein and Kristen Chiafullo and Liam B. Fedus and Noah Fiedel and Rosanne Liu and Vedant Misra and Vinay Venkatesh Ramasesh},
  year   = 2022
}
@article{wei2022chain,
  title   = {Chain of thought prompting elicits reasoning in large language models},
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  year    = 2022,
  journal = {arXiv preprint arXiv:2201.11903}
}
@article{zhang2019dialogpt,
  title   = {Dialogpt: Large-scale generative pre-training for conversational response generation},
  author  = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
  year    = 2019,
  journal = {arXiv preprint arXiv:1911.00536}
}
@article{kuhlen2013language,
  title     = {Language in dialogue: When confederates might be hazardous to your data},
  author    = {Kuhlen, Anna K and Brennan, Susan E},
  year      = 2013,
  journal   = {Psychonomic bulletin \& review},
  publisher = {Springer},
  volume    = 20,
  pages     = {54--72}
}
@article{colman2011distribution,
  title   = {The distribution of repair in dialogue},
  author  = {Colman, Marcus and Healey, Patrick},
  year    = 2011,
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume  = 33,
  number  = 33
}
@article{drieman1962differences,
  title     = {Differences between written and spoken language: An exploratory study},
  author    = {Drieman, Gerard HJ},
  year      = 1962,
  journal   = {Acta Psychologica},
  publisher = {Elsevier},
  volume    = 20,
  pages     = {36--57}
}
@article{chafe1987relation,
  title     = {The relation between written and spoken language},
  author    = {Chafe, Wallace and Tannen, Deborah},
  year      = 1987,
  journal   = {Annual review of anthropology},
  publisher = {Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA},
  volume    = 16,
  number    = 1,
  pages     = {383--407}
}
@book{holler2016turn,
  title     = {Turn-taking in human communicative interaction},
  author    = {Holler, Judith and Kendrick, Kobin H and Casillas, Marisa and Levinson, Stephen C},
  year      = 2016,
  publisher = {Frontiers Media SA}
}
@article{Stolcke_2000,
  title     = {Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech},
  author    = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and Ess-Dykema, Carol Van and Meteer, Marie},
  year      = 2000,
  month     = 9,
  journal   = {Computational Linguistics},
  publisher = {MIT Press - Journals},
  volume    = 26,
  number    = 3,
  pages     = {339–373},
  doi       = {10.1162/089120100561737},
  issn      = {1530-9312},
  url       = {http://dx.doi.org/10.1162/089120100561737}
}
@article{deRuiter2006ProjectingTheEnd,
  title     = {Projecting the end of a speaker's turn: A cognitive cornerstone of conversation},
  author    = {De Ruiter, Jan-Peter and Mitterer, Holger and Enfield, Nick J},
  year      = 2006,
  journal   = {Language},
  publisher = {Linguistic Society of America},
  number    = 3,
  pages     = {515--535}
}
@article{Heldner2010,
  title      = {Pauses, Gaps and Overlaps in Conversations},
  author     = {Mattias Heldner and Jens Edlund},
  year       = 2010,
  journal    = {Journal of Phonetics},
  volume     = 38,
  number     = 4,
  pages      = {555--568},
  doi        = {10.1016/j.wocn.2010.08.002},
  url        = {https://doi.org/10.1016/j.wocn.2010.08.002},
  date_added = {Sat Nov 10 15:24:17 2018}
}
@article{Roberts2006InterTurnSilence,
  title     = {The interaction of inter-turn silence with prosodic cues in listener perceptions of “trouble” in conversation},
  author    = {Roberts, Felicia and Francis, Alexander L. and Morgan, Melanie},
  year      = 2006,
  month     = {Sep},
  journal   = {Speech Communication},
  publisher = {Elsevier BV},
  volume    = 48,
  number    = 9,
  pages     = {1079–1093},
  doi       = {10.1016/j.specom.2006.02.001},
  issn      = {0167-6393},
  url       = {http://dx.doi.org/10.1016/j.specom.2006.02.001}
}
@article{jefferson1983notes,
  title   = {Notes on a possible metric which provides for a standard maximum silence of approximately one second in conversation},
  author  = {Jefferson, Gail},
  year    = 1983,
  journal = {Tilburg papers in language and literature}
}
@inproceedings{michael2020retico,
  title     = {Retico: An incremental framework for spoken dialogue systems},
  author    = {Michael, Thilo},
  year      = 2020,
  booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  pages     = {49--52}
}
@inproceedings{kennington2020rrsds,
  title     = {rrSDS: Towards a robot-ready spoken dialogue system},
  author    = {Kennington, Casey and Moro, Daniele and Marchand, Lucas and Carns, Jake and McNeill, David},
  year      = 2020,
  booktitle = {Proceedings of the 21th annual meeting of the special interest group on discourse and dialogue},
  pages     = {132--135}
}
@article{bohus2010computational,
  title   = {Computational Models for Multiparty Turn Taking},
  author  = {Bohus, Dan and Horvitz, Eric},
  year    = 2010,
  journal = {Technical Report. Microsoft Research Technical Report MSR-TR 2010-115}
}
@article{skantze2021turn,
  title     = {Turn-taking in conversational systems and human-robot interaction: a review},
  author    = {Skantze, Gabriel},
  year      = 2021,
  journal   = {Computer Speech \& Language},
  publisher = {Elsevier},
  volume    = 67,
  pages     = 101178
}
@inproceedings{zhao2018sogo,
  title     = {SOGO: a social intelligent negotiation dialogue system},
  author    = {Zhao, Ran and Romero, Oscar J and Rudnicky, Alex},
  year      = 2018,
  booktitle = {Proceedings of the 18th International Conference on intelligent virtual agents},
  pages     = {239--246}
}
@inproceedings{skantze2009incremental,
  title     = {Incremental dialogue processing in a micro-domain},
  author    = {Skantze, Gabriel and Schlangen, David},
  year      = 2009,
  booktitle = {Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)},
  pages     = {745--753}
}
@inproceedings{skantze2010towards,
  title     = {Towards incremental speech generation in dialogue systems},
  author    = {Skantze, Gabriel and Hjalmarsson, Anna},
  year      = 2010,
  booktitle = {Proceedings of the SIGDIAL 2010 Conference},
  pages     = {1--8}
}
One of the most ubiquitous ways we use to communicate with each other is through informal spoken conversation. In28
such conversations, participants alternate between speaker and listener roles, the assignment of which is locally managed29
by a well-documented set of rules 
@article{sacks1974simplest,
  title     = {A Simplest Systematics for the Organization of Turn-Taking for Conversation},
  author    = {Harvey Sacks and Emanuel A. Schegloff and Gail Jefferson},
  year      = 1974,
  journal   = {Language},
  publisher = {Linguistic Society of America},
  volume    = 50,
  number    = 4,
  pages     = {696--735},
  issn      = {00978507, 15350665},
  url       = {http://www.jstor.org/stable/412243},
  urldate   = {2022-05-05},
  abstract  = {The organization of taking turns to talk is fundamental to conversation, as well as to other speech-exchange systems. A model for the turn-taking organization for conversation is proposed, and is examined for its compatibility with a list of grossly observable facts about conversation. The results of the examination suggest that, at least, a model for turn-taking in conversation will be characterized as locally managed, party-administered, interactionally controlled, and sensitive to recipient design. Several general consequences of the model are explicated, and contrasts are sketched with turn-taking organizations for other speech-exchange systems.}
}
@article{godfrey1993switchboard,
  title   = {Switchboard-1 Release 2 LDC97S62},
  author  = {Godfrey, John and Holliman, Edward},
  year    = 1993,
  journal = {Linguistic Data Consortium}
}
@article{Gronau2020Bridgesampling,
  title     = {bridgesampling: An R Package for Estimating Normalizing Constants},
  author    = {Gronau, Quentin F. and Singmann, Henrik and Wagenmakers, Eric-Jan},
  year      = 2020,
  journal   = {Journal of Statistical Software},
  publisher = {Foundation for Open Access Statistic},
  volume    = 92,
  number    = 10,
  doi       = {10.18637/jss.v092.i10},
  issn      = {1548-7660},
  url       = {http://dx.doi.org/10.18637/jss.v092.i10}
}
@article{Goodrich2020rstanarm,
  title  = {Rstanarm: Bayesian Applied Regression Modeling via Stan R  Package v. 2.19.2.},
  author = {Goodrich, B. and Gabry, J. and Ali, I and Brilleman, S.},
  year   = 2020
}
@article{Duncan1972SomeSignals,
  title     = {Some signals and rules for taking speaking turns in conversations.},
  author    = {Duncan, Starkey},
  year      = 1972,
  journal   = {Journal of Personality and Social Psychology},
  publisher = {American Psychological Association (APA)},
  volume    = 23,
  number    = 2,
  pages     = {283–292},
  doi       = {10.1037/h0033031},
  issn      = {0022-3514},
  url       = {http://dx.doi.org/10.1037/h0033031}
}
@article{Skantze2011Incremental,
  title     = {A General, Abstract Model of Incremental Dialogue Processing},
  author    = {Schlangen, David and Skantze, Gabriel},
  year      = 2011,
  month     = {May},
  journal   = {Dialogue \& Discourse},
  publisher = {University of Illinois Libraries},
  volume    = 2,
  number    = 1,
  pages     = {83–111},
  doi       = {10.5087/dad.2011.105},
  issn      = {2152-9620},
  url       = {http://dx.doi.org/10.5087/dad.2011.105}
}
@book{Clark1996,
  title     = {Using Language},
  author    = {Herbert H. Clark},
  year      = 1996,
  publisher = {Cambridge University Press},
  isbn      = 0521567459
}
@article{Hjalmarsson2008HumanLike,
  title     = {Towards human-like spoken dialogue systems},
  author    = {Edlund, Jens and Gustafson, Joakim and Heldner, Mattias and Hjalmarsson, Anna},
  year      = 2008,
  month     = {Aug},
  journal   = {Speech Communication},
  publisher = {Elsevier BV},
  volume    = 50,
  number    = {8–9},
  pages     = {630–645},
  doi       = {10.1016/j.specom.2008.04.002},
  issn      = {0167-6393},
  url       = {http://dx.doi.org/10.1016/j.specom.2008.04.002}
}
@article{Torreira2015IntonationalBoundaries,
  title     = {Listeners use intonational phrase boundaries to project turn ends in spoken interaction},
  author    = {Bögels, Sara and Torreira, Francisco},
  year      = 2015,
  month     = {Sep},
  journal   = {Journal of Phonetics},
  publisher = {Elsevier BV},
  volume    = 52,
  pages     = {46–57},
  doi       = {10.1016/j.wocn.2015.04.004},
  issn      = {0095-4470},
  url       = {http://dx.doi.org/10.1016/j.wocn.2015.04.004}
}
@inproceedings{lala2019smooth,
  title     = {Smooth turn-taking by a robot using an online continuous model to generate turn-taking cues},
  author    = {Lala, Divesh and Inoue, Koji and Kawahara, Tatsuya},
  year      = 2019,
  booktitle = {2019 International Conference on Multimodal Interaction},
  pages     = {226--234}
}
@article{selting2000construction,
  title     = {The construction of units in conversational talk},
  author    = {Selting, Margret},
  year      = 2000,
  journal   = {Language in society},
  publisher = {Cambridge University Press},
  volume    = 29,
  number    = 4,
  pages     = {477--517}
}
@article{jokinen2013gaze,
  title     = {Gaze and turn-taking behavior in casual conversational interactions},
  author    = {Jokinen, Kristiina and Furukawa, Hirohisa and Nishida, Masafumi and Yamamoto, Seiichi},
  year      = 2013,
  journal   = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
  publisher = {ACM New York, NY, USA},
  volume    = 3,
  number    = 2,
  pages     = {1--30}
}
@article{Holliman1993Switchboard,
  title   = {Switchboard-1 Release 2},
  author  = {Godfrey, John J. \& Holliman, Edward},
  year    = 1993,
  journal = {Linguistic Data Consortium},
  doi     = {https://doi.org/10.35111/SW3H-RW02}
}
@article{Kendrick2015,
  title      = {Never Say No ... How the Brain Interprets the Pregnant Pause in Conversation},
  author     = {Sara Bogels and Kobin H. Kendrick and Stephen C. Levinson},
  year       = 2015,
  journal    = {PLOS ONE},
  volume     = 10,
  number     = 12,
  pages      = {e0145474},
  doi        = {10.1371/journal.pone.0145474},
  url        = {https://doi.org/10.1371/journal.pone.0145474},
  date_added = {Sun Nov 11 13:37:39 2018}
}
@article{Bogels2015IntonationalPhraseBoundaries,
  title     = {Listeners use intonational phrase boundaries to project turn ends in spoken interaction},
  author    = {Bögels, Sara and Torreira, Francisco},
  year      = 2015,
  month     = 9,
  journal   = {Journal of Phonetics},
  publisher = {Elsevier BV},
  volume    = 52,
  pages     = {46–57},
  doi       = {10.1016/j.wocn.2015.04.004},
  issn      = {0095-4470},
  url       = {http://dx.doi.org/10.1016/j.wocn.2015.04.004}
}
@article{Gravano2011TurnTakingCues,
  title     = {Turn-taking cues in task-oriented dialogue},
  author    = {Gravano, Agustín and Hirschberg, Julia},
  year      = 2011,
  month     = {Jul},
  journal   = {Computer Speech \& Language},
  publisher = {Elsevier BV},
  volume    = 25,
  number    = 3,
  pages     = {601–634},
  doi       = {10.1016/j.csl.2010.10.003},
  issn      = {0885-2308},
  url       = {http://dx.doi.org/10.1016/j.csl.2010.10.003}
}
@article{ford1996intonational,
  title     = {intonational, and pragmatic resources for the},
  author    = {Ford, Cecilia E and Thompson, Sandra A},
  year      = 1996,
  journal   = {Interaction and grammar},
  publisher = {Cambridge University Press},
  volume    = 13,
  pages     = 134
}
@inproceedings{gervits2020AboutTime,
  title     = {It's About Time: Turn-Entry Timing for Situated Human-Robot Dialogue},
  author    = {Felix Gervits and Ravenna Thielstrom and Antonio Roque and Matthias Scheutz},
  year      = 2020,
  booktitle = {Proceedings of the Special Interest Group on Discourse and Dialogue},
  topics    = {hri, arch, dia, comm},
  projects  = {cm, itl},
  synopsis  = {We introduce a computational framework, based on work from psycholinguistics, which is aimed at achieving proper turn-entry timing for situated agents.}
}
@article{stivers2009universals,
  title     = {Universals and cultural variation in turn-taking in conversation},
  author    = {Stivers, Tanya and Enfield, Nicholas J and Brown, Penelope and Englert, Christina and Hayashi, Makoto and Heinemann, Trine and Hoymann, Gertie and Rossano, Federico and De Ruiter, Jan P. and Yoon, Kyung-Eun and others},
  year      = 2009,
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {National Acad Sciences},
  volume    = 106,
  number    = 26,
  pages     = {10587--10592}
}
@article{edlund2008towards,
  title     = {Towards human-like spoken dialogue systems},
  author    = {Edlund, Jens and Gustafson, Joakim and Heldner, Mattias and Hjalmarsson, Anna},
  year      = 2008,
  journal   = {Speech communication},
  publisher = {Elsevier},
  volume    = 50,
  number    = {8-9},
  pages     = {630--645}
}
@article{wetzels2011statistical,
  title     = {Statistical evidence in experimental psychology: An empirical comparison using 855 t tests},
  author    = {Wetzels, Ruud and Matzke, Dora and Lee, Michael D and Rouder, Jeffrey N and Iverson, Geoffrey J and Wagenmakers, Eric-Jan},
  year      = 2011,
  journal   = {Perspectives on Psychological Science},
  publisher = {Sage Publications Sage CA: Los Angeles, CA},
  volume    = 6,
  number    = 3,
  pages     = {291--298}
}
@inproceedings{masumura2018neural,
  title     = {Neural dialogue context online end-of-turn detection},
  author    = {Masumura, Ryo and Tanaka, Tomohiro and Ando, Atsushi and Ishii, Ryo and Higashinaka, Ryuichiro and Aono, Yushi},
  year      = 2018,
  booktitle = {Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue},
  pages     = {224--228}
}
@inproceedings{masumura2019improving,
  title        = {Improving speech-based end-of-turn detection via cross-modal representation learning with punctuated text data},
  author       = {Masumura, Ryo and Ihori, Mana and Tanaka, Tomohiro and Ando, Atsushi and Ishii, Ryo and Oba, Takanobu and Higashinaka, Ryuichiro},
  year         = 2019,
  booktitle    = {2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages        = {1062--1069},
  organization = {IEEE}
}
@article{lemoine2019moving,
  title     = {Moving beyond noninformative priors: why and how to choose weakly informative priors in Bayesian analyses},
  author    = {Lemoine, Nathan P},
  year      = 2019,
  journal   = {Oikos},
  publisher = {Wiley Online Library},
  volume    = 128,
  number    = 7,
  pages     = {912--928}
}
@article{levinson2015timing,
  title     = {Timing in turn-taking and its implications for processing models of language},
  author    = {Levinson, Stephen C and Torreira, Francisco},
  year      = 2015,
  journal   = {Frontiers in psychology},
  publisher = {Frontiers},
  volume    = 6,
  pages     = 731
}
@article{stivers2006preference,
  title     = {A preference for progressivity in interaction},
  author    = {Stivers, Tanya and Robinson, Jeffrey D},
  year      = 2006,
  journal   = {Language in society},
  publisher = {Cambridge University Press},
  volume    = 35,
  number    = 3,
  pages     = {367--392}
}
@article{liddicoat2004projectability,
  title     = {The projectability of turn constructional units and the role of prediction in listening},
  author    = {Liddicoat, Anthony J},
  year      = 2004,
  journal   = {Discourse Studies},
  publisher = {Sage Publications London, Thousand Oaks, CA and New Delhi},
  volume    = 6,
  number    = 4,
  pages     = {449--469}
}
@inproceedings{hara2019turn,
  title     = {Turn-Taking Prediction Based on Detection of Transition Relevance Place.},
  author    = {Hara, Kohei and Inoue, Koji and Takanashi, Katsuya and Kawahara, Tatsuya},
  year      = 2019,
  booktitle = {INTERSPEECH},
  pages     = {4170--4174}
}
@inproceedings{bosch2004durational,
  title        = {Durational aspects of turn-taking in spontaneous face-to-face and telephone dialogues},
  author       = {Bosch, Louis ten and Oostdijk, Nelleke and De Ruiter, Jan-Peter},
  year         = 2004,
  booktitle    = {International Conference on Text, Speech and Dialogue},
  pages        = {563--570},
  organization = {Springer}
}
@article{french1990subcognition,
  title     = {Subcognition and the limits of the Turing test},
  author    = {French, Robert M},
  year      = 1990,
  journal   = {Mind},
  publisher = {JSTOR},
  volume    = 99,
  number    = 393,
  pages     = {53--65}
}
@article{ni2022recent,
  title     = {Recent advances in deep learning based dialogue systems: A systematic survey},
  author    = {Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and Cambria, Erik},
  year      = 2022,
  journal   = {Artificial intelligence review},
  publisher = {Springer},
  pages     = {1--101}
}
@inproceedings{helwe2021reasoning,
  title     = {Reasoning with transformer-based models: Deep learning, but shallow reasoning},
  author    = {Helwe, Chadi and Clavel, Chlo{\'e} and Suchanek, Fabian M},
  year      = 2021,
  booktitle = {3rd conference on automated knowledge base construction}
}
@article{raffel2020exploring,
  title     = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  year      = 2020,
  journal   = {The Journal of Machine Learning Research},
  publisher = {JMLRORG},
  volume    = 21,
  number    = 1,
  pages     = {5485--5551}
}
@article{van2008neural,
  title     = {The neural integration of speaker and message},
  author    = {Van Berkum, Jos JA and Van den Brink, Danielle and Tesink, Cathelijne MJY and Kos, Miriam and Hagoort, Peter},
  journal   = {Journal of cognitive neuroscience},
  volume    = {20},
  number    = {4},
  pages     = {580--591},
  year      = {2008},
  publisher = {MIT Press}
}
@article{metzing2003conceptual,
  title     = {When conceptual pacts are broken: Partner-specific effects on the comprehension of referring expressions},
  author    = {Metzing, Charles and Brennan, Susan E},
  journal   = {Journal of Memory and Language},
  volume    = {49},
  number    = {2},
  pages     = {201--213},
  year      = {2003},
  publisher = {Elsevier}
}
@book{bird2009natural,
  title     = {Natural language processing with Python: analyzing text with the natural language toolkit},
  author    = {Bird, Steven and Klein, Ewan and Loper, Edward},
  year      = 2009,
  publisher = {" O'Reilly Media, Inc."}
}
@article{matsuki2011event,
  title     = {Event-based plausibility immediately influences on-line language comprehension.},
  author    = {Matsuki, Kazunaga and Chow, Tracy and Hare, Mary and Elman, Jeffrey L and Scheepers, Christoph and McRae, Ken},
  journal   = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume    = {37},
  number    = {4},
  pages     = {913},
  year      = {2011},
  publisher = {American Psychological Association}
}
@inproceedings{wolf2019huggingface,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Wolf, Thomas  and Debut, Lysandre  and Sanh, Victor  and Chaumond, Julien  and Delangue, Clement  and Moi, Anthony  and Cistac, Pierric  and Rault, Tim  and Louf, Remi  and Funtowicz, Morgan  and Davison, Joe  and Shleifer, Sam  and von Platen, Patrick  and Ma, Clara  and Jernite, Yacine  and Plu, Julien  and Xu, Canwen  and Le Scao, Teven  and Gugger, Sylvain  and Drame, Mariama  and Lhoest, Quentin  and Rush, Alexander},
  year      = 2020,
  month     = oct,
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  publisher = {Association for Computational Linguistics},
  address   = {Online},
  pages     = {38--45},
  doi       = {10.18653/v1/2020.emnlp-demos.6},
  url       = {https://aclanthology.org/2020.emnlp-demos.6},
  editor    = {Liu, Qun  and Schlangen, David},
  abstract  = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.}
}
@software{Falcon_PyTorch_Lightning_2019,
  title   = {{PyTorch Lightning}},
  author  = {Falcon, William and {The PyTorch Lightning team}},
  year    = 2019,
  month   = 3,
  doi     = {10.5281/zenodo.3828935},
  url     = {https://github.com/Lightning-AI/lightning},
  license = {Apache-2.0},
  version = {1.4}
}
@article{shannon2001mathematical,
  title   = {A mathematical theory of communication},
  author  = {Shannon, C. E.},
  year    = 1948,
  journal = {The Bell System Technical Journal},
  volume  = 27,
  number  = 3,
  pages   = {379--423},
  doi     = {10.1002/j.1538-7305.1948.tb01338.x}
}
One of the most ubiquitous ways we use to communicate with each other is through informal spoken conversation. In28
such conversations, participants alternate between speaker and listener roles, the assignment of which is locally managed29
by a well-documented set of rules 
@book{levinson_1983,
  title     = {Pragmatics},
  author    = {Levinson, Stephen C.},
  year      = 1983,
  publisher = {Cambridge University Press},
  series    = {Cambridge Textbooks in Linguistics},
  isbn      = 9780198520115
}
@article{topal2021exploring,
  title   = {Exploring transformers in natural language generation: Gpt, bert, and xlnet},
  author  = {Topal, M Onat and Bas, Anil and van Heerden, Imke},
  year    = 2021,
  journal = {arXiv preprint arXiv:2102.08036}
}
@article{lmer,
  title   = {Fitting Linear Mixed-Effects Models Using {lme4}},
  author  = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
  year    = 2015,
  journal = {Journal of Statistical Software},
  volume  = 67,
  number  = 1,
  pages   = {1--48},
  doi     = {10.18637/jss.v067.i01}
}
@article{brms,
  title    = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
  author   = {Paul-Christian Bürkner},
  year     = 2017,
  journal  = {Journal of Statistical Software},
  volume   = 80,
  number   = 1,
  pages    = {1--28},
  doi      = {10.18637/jss.v080.i01},
  encoding = {UTF-8}
}
@inproceedings{zandie2020emptransfo,
  title     = {EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic Dialog Systems},
  author    = {Zandie, Rohola and Mahoor, Mohammad H},
  year      = 2020,
  booktitle = {The Thirty-Third International Flairs Conference}
}
@inproceedings{karita2019comparative,
  title        = {A comparative study on transformer vs rnn in speech applications},
  author       = {Karita, Shigeki and Chen, Nanxin and Hayashi, Tomoki and Hori, Takaaki and Inaguma, Hirofumi and Jiang, Ziyan and Someki, Masao and Soplin, Nelson Enrique Yalta and Yamamoto, Ryuichi and Wang, Xiaofei and others},
  year         = 2019,
  booktitle    = {2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages        = {449--456},
  organization = {IEEE}
}
@article{tilley2003challenging,
  title     = {“Challenging” research practices: Turning a critical lens on the work of transcription},
  author    = {Tilley, Susan A},
  year      = 2003,
  journal   = {Qualitative inquiry},
  publisher = {Sage Publications},
  volume    = 9,
  number    = 5,
  pages     = {750--773}
}
@article{binz2023using,
  title     = {Using cognitive psychology to understand GPT-3},
  author    = {Binz, Marcel and Schulz, Eric},
  year      = 2023,
  journal   = {Proceedings of the National Academy of Sciences},
  publisher = {National Acad Sciences},
  volume    = 120,
  number    = 6,
  pages     = {e2218523120}
}
@article{lund2023chatting,
  title     = {Chatting about ChatGPT: how may AI and GPT impact academia and libraries?},
  author    = {Lund, Brady D and Wang, Ting},
  year      = 2023,
  journal   = {Library Hi Tech News},
  publisher = {Emerald Publishing Limited},
  volume    = 40,
  number    = 3,
  pages     = {26--29}
}
@inproceedings{dou2021gpt,
  title     = {Is {GPT}-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text},
  author    = {Dou, Yao  and Forbes, Maxwell  and Koncel-Kedziorski, Rik  and Smith, Noah A.  and Choi, Yejin},
  year      = 2022,
  month     = may,
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Dublin, Ireland},
  pages     = {7250--7274},
  doi       = {10.18653/v1/2022.acl-long.501},
  url       = {https://aclanthology.org/2022.acl-long.501},
  editor    = {Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline},
  abstract  = {Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow{---}such as redundancy, commonsense errors, and incoherence{---}are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at \url{https://yao-dou.github.io/scarecrow/}.}
}
@inproceedings{bender2021dangers,
  title     = {On the dangers of stochastic parrots: Can language models be too big?},
  author    = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  year      = 2021,
  booktitle = {Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages     = {610--623}
}
@inproceedings{felkner2306winoqueer,
  title     = {{W}ino{Q}ueer: A Community-in-the-Loop Benchmark for Anti-{LGBTQ}+ Bias in Large Language Models},
  author    = {Felkner, Virginia  and Chang, Ho-Chun Herbert  and Jang, Eugene  and May, Jonathan},
  year      = 2023,
  month     = jul,
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Toronto, Canada},
  pages     = {9126--9140},
  doi       = {10.18653/v1/2023.acl-long.507},
  url       = {https://aclanthology.org/2023.acl-long.507},
  editor    = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
  abstract  = {We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.}
}
@inproceedings{nass1994computers,
  title     = {Computers are social actors},
  author    = {Nass, Clifford and Steuer, Jonathan and Tauber, Ellen R},
  year      = 1994,
  booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages     = {72--78}
}
@book{weizenbaum1976computer,
  title     = {Computer Power and Human Reason: From Judgment to Calculation},
  author    = {Weizenbaum, Joseph},
  year      = 1976,
  publisher = {W. H. Freeman \& Co.},
  address   = {USA},
  isbn      = {0716704641},
  abstract  = {From the Publisher:Computer Power and Human Reason is a distinguished computer scientist's elucidation of the impact of scientific rationality on man's self-image.}
}
@misc{pasquinelli2020machine,
  title  = {How a Machine Learns and Fails--A Grammar of Error for Artificial Intelligence. Spheres},
  author = {Pasquinelli, Matteo},
  year   = 2020
}
@inproceedings{liesenfeld2023opening,
  title     = {Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators},
  author    = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  year      = 2023,
  booktitle = {Proceedings of the 5th international conference on conversational user interfaces},
  pages     = {1--6}
}
@inproceedings{li2022pretrained,
  title     = {Pretrained Language Model for Text Generation: A Survey},
  author    = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wen, Ji-Rong},
  year      = 2021,
  month     = 8,
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {4492--4499},
  doi       = {10.24963/ijcai.2021/612},
  url       = {https://doi.org/10.24963/ijcai.2021/612},
  note      = {Survey Track},
  editor    = {Zhi-Hua Zhou}
}
@article{touvron2023llama,
  title   = {Llama 2: Open foundation and fine-tuned chat models},
  author  = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  year    = 2023,
  journal = {arXiv preprint arXiv:2307.09288}
}

% Technical report for GPT-4 
@article{achiam2023gpt,
  title   = {Gpt-4 technical report},
  author  = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal = {arXiv preprint arXiv:2303.08774},
  year    = {2023}
}
@article{levy2008expectation,
  title     = {Expectation-based syntactic comprehension},
  author    = {Levy, Roger},
  year      = 2008,
  journal   = {Cognition},
  publisher = {Elsevier},
  volume    = 106,
  number    = 3,
  pages     = {1126--1177}
}
@book{mertens2022miscommunication,
  title     = {Miscommunication: How Prediction and Egocentricity Increase Progressivity but Decrease Intersubjectivity},
  author    = {Mertens, Julia B},
  year      = 2022,
  publisher = {Tufts University}
}
@article{xu2023retrieval,
  title   = {Retrieval meets long context large language models},
  author  = {Xu, Peng and Ping, Wei and Wu, Xianchao and McAfee, Lawrence and Zhu, Chen and Liu, Zihan and Subramanian, Sandeep and Bakhturina, Evelina and Shoeybi, Mohammad and Catanzaro, Bryan},
  year    = 2023,
  journal = {arXiv preprint arXiv:2310.03025}
}
@article{meyer2023chatgpt,
  title     = {ChatGPT and large language models in academia: opportunities and challenges},
  author    = {Meyer, Jesse G and Urbanowicz, Ryan J and Martin, Patrick CN and O’Connor, Karen and Li, Ruowang and Peng, Pei-Chen and Bright, Tiffani J and Tatonetti, Nicholas and Won, Kyoung Jae and Gonzalez-Hernandez, Graciela and others},
  year      = 2023,
  journal   = {BioData Mining},
  publisher = {Springer},
  volume    = 16,
  number    = 1,
  pages     = 20
}
@article{sejnowski2023large,
  title     = {Large language models and the reverse turing test},
  author    = {Sejnowski, Terrence J},
  year      = 2023,
  journal   = {Neural computation},
  publisher = {MIT Press},
  volume    = 35,
  number    = 3,
  pages     = {309--342}
}
@article{schegloff1982discourse,
  title   = {Discourse as an interactional achievement: Some uses of ‘uh huh’and other things that come between sentences},
  author  = {Schegloff, Emanuel A},
  year    = 1982,
  journal = {Analyzing discourse: Text and talk},
  volume  = 71,
  pages   = {71--93}
}
@article{besacier2014automatic,
  title     = {Automatic speech recognition for under-resourced languages: A survey},
  author    = {Besacier, Laurent and Barnard, Etienne and Karpov, Alexey and Schultz, Tanja},
  year      = 2014,
  journal   = {Speech communication},
  publisher = {Elsevier},
  volume    = 56,
  pages     = {85--100}
}
@inproceedings{schofield-etal-2017-pulling,
  title     = {Pulling Out the Stops: Rethinking Stopword Removal for Topic Models},
  author    = {Schofield, Alexandra  and Magnusson, M{\aa}ns  and Mimno, David},
  year      = 2017,
  month     = apr,
  booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  publisher = {Association for Computational Linguistics},
  address   = {Valencia, Spain},
  pages     = {432--436},
  url       = {https://aclanthology.org/E17-2069},
  editor    = {Lapata, Mirella  and Blunsom, Phil  and Koller, Alexander},
  abstract  = {It is often assumed that topic models benefit from the use of a manually curated stopword list. Constructing this list is time-consuming and often subject to user judgments about what kinds of words are important to the model and the application. Although stopword removal clearly affects which word types appear as most probable terms in topics, we argue that this improvement is superficial, and that topic inference benefits little from the practice of removing stopwords beyond very frequent terms. Removing corpus-specific stopwords after model inference is more transparent and produces similar results to removing those words prior to inference.}
}
@article{pham2001mean,
  title     = {The mean and median absolute deviations},
  author    = {Pham-Gia, Thu and Hung, Tran Loc},
  year      = 2001,
  journal   = {Mathematical and computer Modelling},
  publisher = {Elsevier},
  volume    = 34,
  number    = {7-8},
  pages     = {921--936}
}
@inproceedings{qiao-etal-2023-reasoning,
  title     = {Reasoning with Language Model Prompting: A Survey},
  author    = {Qiao, Shuofei  and Ou, Yixin  and Zhang, Ningyu  and Chen, Xiang  and Yao, Yunzhi  and Deng, Shumin  and Tan, Chuanqi  and Huang, Fei  and Chen, Huajun},
  year      = 2023,
  month     = jul,
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  publisher = {Association for Computational Linguistics},
  address   = {Toronto, Canada},
  pages     = {5368--5393},
  doi       = {10.18653/v1/2023.acl-long.294},
  url       = {https://aclanthology.org/2023.acl-long.294},
  editor    = {Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki},
  abstract  = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at \url{https://github.com/zjunlp/Prompt4ReasoningPapers} (updated periodically).}
}
@inproceedings{guo-etal-2022-longt5,
  title     = {{L}ong{T}5: {E}fficient Text-To-Text Transformer for Long Sequences},
  author    = {Guo, Mandy  and Ainslie, Joshua  and Uthus, David  and Ontanon, Santiago  and Ni, Jianmo  and Sung, Yun-Hsuan  and Yang, Yinfei},
  year      = 2022,
  month     = jul,
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
  publisher = {Association for Computational Linguistics},
  address   = {Seattle, United States},
  pages     = {724--736},
  doi       = {10.18653/v1/2022.findings-naacl.55},
  url       = {https://aclanthology.org/2022.findings-naacl.55},
  editor    = {Carpuat, Marine  and de Marneffe, Marie-Catherine  and Meza Ruiz, Ivan Vladimir},
  abstract  = {Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC{'}s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.}
}


Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.
@article{solaiman2019release,
  title   = {Release strategies and the social impacts of language models},
  author  = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal = {arXiv preprint arXiv:1908.09203},
  year    = {2019}
}

% Introduction paper for the LLama model. 
@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

% Introduction for Google Gemini
@article{reid2024gemini,
  title   = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author  = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal = {arXiv preprint arXiv:2403.05530},
  year    = {2024}
}

% Points out that LLMs are being used in media in both good and bad ways. 
@article{cheng2024journalism,
  title     = {When Journalism Meets AI: Risk or Opportunity?},
  author    = {Cheng, Sophia},
  journal   = {Digital Government: Research and Practice},
  year      = {2024},
  publisher = {ACM New York, NY}
}

% Use of LLMs as programming partners. 
@inproceedings{finnie2022robots,
  title     = {The robots are coming: Exploring the implications of openai codex on introductory programming},
  author    = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A and Luxton-Reilly, Andrew and Prather, James},
  booktitle = {Proceedings of the 24th Australasian Computing Education Conference},
  pages     = {10--19},
  year      = {2022}
}

% Documents the effects of RLHF in LLMs. 
@article{kirk2023understanding,
  title   = {Understanding the effects of rlhf on llm generalisation and diversity},
  author  = {Kirk, Robert and Mediratta, Ishita and Nalmpantis, Christoforos and Luketina, Jelena and Hambro, Eric and Grefenstette, Edward and Raileanu, Roberta},
  journal = {arXiv preprint arXiv:2310.06452},
  year    = {2023}
}

% Demonstrates that while open-source is a term associated with modern LLMs., they are in fact not as open source as we think (including in terms of the training data)
@inproceedings{liesenfeld2024rethinking,
  title        = {Rethinking open source generative AI: open-washing and the EU AI Act},
  author       = {Liesenfeld, Andreas and Dingemanse, Mark},
  booktitle    = {Seventh Annual ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT 2024)},
  year         = {2024},
  organization = {ACM}
}


% Survey on the development and history of LLMs and the various tasks they can do. Used to show that LLMs are developing very fast. 
@article{yang2024harnessing,
  author     = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  title      = {Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond},
  year       = {2024},
  issue_date = {July 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {18},
  number     = {6},
  issn       = {1556-4681},
  url        = {https://doi.org/10.1145/3649506},
  doi        = {10.1145/3649506},
  abstract   = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at . An LLMs evolutionary tree, editable yet regularly updated, can be found at .},
  journal    = {ACM Trans. Knowl. Discov. Data},
  month      = {apr},
  articleno  = {160},
  numpages   = {32},
  keywords   = {Large language models, neural language processing, practical guide, ChatGPT}
}
 

% Google's LAMDA introduction paper. 
@incollection{thoppilan2022lamda,
  title     = {LaMDA: Language Models for Dialog Applications},
  author    = {Aaron Daniel Cohen and Adam Roberts and Alejandra Molina and Alena Butryna and Alicia Jin and Apoorv Kulshreshtha and Ben Hutchinson and Ben Zevenbergen and Blaise Hilary Aguera-Arcas and Chung-ching Chang and Claire Cui and Cosmo Du and Daniel De Freitas Adiwardana and Dehao Chen and Dmitry (Dima) Lepikhin and Ed H. Chi and Erin Hoffman-John and Heng-Tze Cheng and Hongrae Lee and Igor Krivokon and James Qin and Jamie Hall and Joe Fenton and Johnny Soraker and Kathy Meier-Hellstern and Kristen Olson and Lora Mois Aroyo and Maarten Paul Bosma and Marc Joseph Pickett and Marcelo Amorim Menegali and Marian Croak and Mark Díaz and Matthew Lamm and Maxim Krikun and Meredith Ringel Morris and Noam Shazeer and Quoc V. Le and Rachel Bernstein and Ravi Rajakumar and Ray Kurzweil and Romal Thoppilan and Steven Zheng and Taylor Bos and Toju Duke and Tulsee Doshi and Vincent Y. Zhao and Vinodkumar Prabhakaran and Will Rusch and YaGuang Li and Yanping Huang and Yanqi Zhou and Yuanzhong Xu and Zhifeng Chen},
  year      = {2022},
  booktitle = {arXiv}
}

% Argues that syntetic data for larger LLMs does not mean that they will necessarily perform better,. 
@article{gholami2023generative,
  title   = {Do Generative Large Language Models need billions of parameters?},
  author  = {Gholami, Sia and Omar, Marwan},
  journal = {arXiv preprint arXiv:2309.06589},
  year    = {2023}
}

% Larger datasets lead to dimishing returns. 
@article{shumailov2023curse,
  title   = {The curse of recursion: Training on generated data makes models forget},
  author  = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
  journal = {arXiv preprint arXiv:2305.17493},
  year    = {2023}
}

% Argues that LLMs are not available equally to people and in fact their power is highly concentrated to a few countries and companies. 
@article{sathish2024llempower,
  title   = {LLeMpower: Understanding Disparities in the Control and Access of Large Language Models},
  author  = {Sathish, Vishwas and Lin, Hannah and Kamath, Aditya K and Nyayachavadi, Anish},
  journal = {arXiv preprint arXiv:2404.09356},
  year    = {2024}
}


Enterprises need to fine-tune Large Language Models (LLMs) on proprietary domain knowledge efficiently. While Retrieval Augmented Generation (RAG) offers an alternative, it is limited by vector database quality. This work focuses on fine-tuning LLaMA using proprietary documents and code, evaluating response quality, and providing practical guidelines for beginners on GPU requirements and data formatting. It includes preprocessing methods for documents (e.g., paragraph chunks, Q&A pairs) and code (e.g., summary-function pairs), with qualitative evaluations and recommendations for fine-tuning LLMs.
@article{vm2024fine,
  title   = {Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations},
  author  = {VM, Kushala and Warrier, Harikrishna and Gupta, Yogesh and others},
  journal = {arXiv preprint arXiv:2404.10779},
  year    = {2024}
}


% We do not know exactly what data is used to train state-of-the-art LLMs. 
@article{balloccu2024leak,
  title   = {Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms},
  author  = {Balloccu, Simone and Schmidtov{\'a}, Patr{\'\i}cia and Lango, Mateusz and Du{\v{s}}ek, Ond{\v{r}}ej},
  journal = {arXiv preprint arXiv:2402.03927},
  year    = {2024}
}

This paper investigates fine-tuning LLMs on domain-specific dialogue data and reports a decrease in performance when the pre-training and fine-tuning data distributions do not align, emphasizing the difficulty in achieving consistent accuracy across different data types.
@article{sun2024dial,
  title   = {Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse},
  author  = {Sun, Jianwei and Mei, Chaoyang and Wei, Linlin and Zheng, Kaiyu and Liu, Na and Cui, Ming and Li, Tianyi},
  journal = {arXiv preprint arXiv:2403.09167},
  year    = {2024}
}

This paper investigates fine-tuning LLMs on domain-specific dialogue data and reports a decrease in performance when the pre-training and fine-tuning data distributions do not align, emphasizing the difficulty in achieving consistent accuracy across different data types.
@article{yang2024unveiling,
  title   = {Unveiling the generalization power of fine-tuned large language models},
  author  = {Yang, Haoran and Zhang, Yumeng and Xu, Jiaqi and Lu, Hongyuan and Heng, Pheng Ann and Lam, Wai},
  journal = {arXiv preprint arXiv:2403.09162},
  year    = {2024}
}

Highlights the correlation between word-level and sentence-level predictability.
@article{smith2013effect,
  title     = {The effect of word predictability on reading time is logarithmic},
  author    = {Smith, Nathaniel J and Levy, Roger},
  journal   = {Cognition},
  volume    = {128},
  number    = {3},
  pages     = {302--319},
  year      = {2013},
  publisher = {Elsevier}
}

Demonstrates the alignment of word-level and sentence-level surprisal in short contexts.
@article{frank2011insensitivity,
  title     = {Insensitivity of the human sentence-processing system to hierarchical structure},
  author    = {Frank, Stefan L and Bod, Rens},
  journal   = {Psychological science},
  volume    = {22},
  number    = {6},
  pages     = {829--834},
  year      = {2011},
  publisher = {Sage Publications Sage CA: Los Angeles, CA}
}

focuses on using LSTM Recurrent Neural Networks for modeling turn-taking in spoken dialogue. The study emphasizes the need for continuous models that can handle turn transitions. By preprocessing data to include explicit EOT tokens, we align with the approach of training models to better understand and predict turn transitions, thus improving their performance in dialogue systems.
@inproceedings{skantze2017towards,
  title     = {Towards a general, continuous model of turn-taking in spoken dialogue using LSTM recurrent neural networks},
  author    = {Skantze, Gabriel},
  booktitle = {Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  pages     = {220--230},
  year      = {2017}
}


% Extension of turngpt that conditions predictions on responses. 
@inproceedings{jiang-etal-2023-response,
  title     = {Response-conditioned Turn-taking Prediction},
  author    = {Jiang, Bing{'}er  and
               Ekstedt, Erik  and
               Skantze, Gabriel},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.776},
  doi       = {10.18653/v1/2023.findings-acl.776},
  pages     = {12241--12248},
  abstract  = {Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings.}
}

% Example of a model that has been developed to detect the end of turns using acoustic and linguistic features. 
@inproceedings{ekstedt2022much,
  author    = {Erik Ekstedt and Gabriel Skantze},
  title     = {{Voice Activity Projection: Self-supervised Learning of Turn-taking Events}},
  year      = 2022,
  booktitle = {Proc. Interspeech 2022},
  pages     = {5190--5194},
  doi       = {10.21437/Interspeech.2022-10955}
}