% NOTATION
%%%%%%%%%%%%

% general math
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\expectedvalue}{\mathbb{E}_{\policy}}
\newcommand{\true}{\mathit{true}}
\newcommand{\false}{\mathit{false}}


% STRIPS
\newcommand{\task}{T}
\newcommand{\fluents}{F}
\newcommand{\literals}{L}
\newcommand{\operators}{O}
\newcommand{\initfluentstate}{\sigma_0}
\newcommand{\goal}{\partialfluentstate_g}
\newcommand{\subgoal}{\partialfluentstate_{sg}}
\newcommand{\subgoals}{\Sigma_{sg}}
\newcommand{\fluent}{f}
\newcommand{\literal}{l}
\newcommand{\fluentstate}{\sigma}
\newcommand{\partialfluentstate}{\tilde{\sigma}}
\newcommand{\operator}{o}
\newcommand{\partialoperator}{\tilde{o}}
\newcommand{\plan}{\pi_T}
\newcommand{\precon}{pre}
\newcommand{\effect}{\mathit{eff}}
\newcommand{\addeffect}{\mathit{eff}^+}
\newcommand{\deleffect}{\mathit{eff}^-}
\newcommand{\static}{static}
\newcommand{\unknown}{unknown}
\newcommand{\allliterals}{\mathcal{L}(\fluents)}
\newcommand{\fluenttransition}{\delta(\partialfluentstate, \operator)}
\newcommand{\applyoperator}[2]{\delta(#1, #2)}
\newcommand{\regressoperator}[2]{\delta^{-1}(#1, #2)}
\newcommand{\applyfunction}{\delta}
\newcommand{\regressfunction}{\delta^{-1}}

\newcommand{\reachable}{\Delta_{\partialfluentstate}}
\newcommand{\planstates}{\Sigma_{reach}}
\newcommand{\unplanstates}{\Sigma_{dead}}
\newcommand{\restrict}{restrict}

\newcommand{\planfluentstates}{\Sigma_{plan}}
\newcommand{\reachfluentstates}{\Sigma_{reach}}
\newcommand{\deadfluentstates}{\Sigma_{dead}}


% MDP
\newcommand{\mdp}{M}
\newcommand{\mstates}{S}
\newcommand{\actions}{A}
\newcommand{\rewardfunction}{r}
\newcommand{\transitionprob}{p}
\newcommand{\discountfactor}{\gamma}
\newcommand{\mstate}{s}
\newcommand{\initialstatedist}{\iota}
\newcommand{\initialstate}{\mstate_0}
\newcommand{\action}{a}
\newcommand{\valuefunction}{v_{\policy(\mstate)}}
\newcommand{\policy}{\pi_M}
\newcommand{\policies}{\Pi_M}
\newcommand{\achievable}{\Omega_{\mstate}}
\newcommand{\qfunction}{q(\mstate, \action)}
\newcommand{\qfunctionopt}{q^*(\mstate, \action)}
\newcommand{\qfunctionnext}{q(\newstate, \newaction)}
\newcommand{\learningrate}{\alpha}
\newcommand{\experience}{(\mstate, \action, \rewardfunction, \newstate}
\newcommand{\newstate}{\mstate^\prime}
\newcommand{\newaction}{\action^\prime}
\newcommand{\onlinelearner}{\ell_{expl}}
\newcommand{\offlinelearner}{\ell}
\newcommand{\learners}{L}
\newcommand{\threshold}{\tau}

\newcommand{\exec}{x}
\newcommand{\execinit}{I_x}
\newcommand{\execpolicy}{\pi_x}
\newcommand{\execterm}{\beta_x}
\newcommand{\execs}{X}

\newcommand{\execstar}{x^\star}
\newcommand{\execinitstar}{I_{x^\star}}
\newcommand{\execpolicystar}{\pi_{x^\star}}
\newcommand{\exectermstar}{\beta_{x^\star}}

\newcommand{\execinitfor}[1]{I_{#1}}
\newcommand{\execpolicyfor}[1]{\pi_{#1}}
\newcommand{\exectermfor}[1]{\beta_{#1}}

\newcommand{\explorationpolicy}{\pi_{expl}}

% Interated stuff
\newcommand{\symdp}{\mathcal{T}}
\newcommand{\detector}{d}
\newcommand{\executor}{e}
\newcommand{\macgyver}{\tilde{\symdp}}

% Learning preconditions
\newcommand{\applicablefluentstates}{\Sigma_{app}}
\newcommand{\beenfluentstates}{\Sigma_{been}}
\newcommand{\abovethresholdfluentstates}{\Sigma_{>\tau}}
\newcommand{\node}{node}
\newcommand{\common}{\partialfluentstate_{common}}
\newcommand{\anotherfluentstate}{\fluentstate^\prime}



% Target MDP
\newcommand{\targetmdp}{M_T}
\newcommand{\bsn}{\subgoal}

% Agent level
\newcommand{\lflag}{\mathit{impasse}}

% Algorithm stuff
\newcommand{\solve}{\textbf{\textsc{solve}}}
\newcommand{\learn}{\textbf{\textsc{learn}}}
\newcommand{\genprecon}{\textbf{\textsc{gen-precon}}}
\newcommand{\execute}{\textsc{execute}}
\newcommand{\env}{\mbox{ENV}}
\newcommand{\owfs}{\textsc{owfs}}
\newcommand{\owbs}{\textsc{owbs}}

