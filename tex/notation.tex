% % NOTATION
% %%%%%%%%%%%%

% Here is we should define our own custom commands!!! 

% Define additional settings and commands
\newtheorem{hyp}{Hypothesis}
\newtheorem{subhyp}{Hypothesis}
\renewcommand\thesubhyp{\thehyp\alph{subhyp}}

% Here are the various links to the repositories. 
\newcommand{\repolink}{\url{https://github.com/mumair01/GPT-Monologue-to-Dialogue}}
\newcommand{\anonymousrepolink}{\url{https://anonymous.4open.science/r/GPT-Monologue-to-Dialogue-49B3/README.md}}
\newcommand{\osflink}{\url{https://osf.io/fxn8y/?view_only=9baf4033a2cb49cfaf107f9a753ab445}}
\newcommand{\anonymousosflink}{\url{https://osf.io/fxn8y/?view_only=c70f49ab84a149b6b999be606f619eb2}}

% This is the notation for words and surprisal used in this work. 
% \newcommand{\word}[1]{w_{#1}}
\newcommand{\token}[1]{\mathit{t_{#1}}}

\NewDocumentCommand{\word}{m o}{%
    w_{#1}%
    \IfValueT{#2}{^{#2}}%
}
\newcommand{\sequence}{\mathit{S}}
\newcommand{\wordSurprisal}{\mathit{Surprisal}}


\newcommand{\secondTurnSurprisal}[1]{\wordSurprisal^{\mathit{#1}}_{\mathit{secondTurn}}}


\newcommand{\wordSurprisalFormula}{-\log P(\word{i} \mid \word{1}, \ldots, \word{i-1})}
\newcommand{\tokenSurprisalFormula}{-\log P(\token{i} \mid \token{1}, \ldots, \token{i-1})}
\newcommand{\secondTurnWordsSurprisalFormula}{\sum_{i=1}^{N} - \log P(\word{i}[2] \mid \word{1}[2], \ldots, \word{i-1}[2], \word{1}[1], \ldots, \word{K}[1])}
\newcommand{\SecondTurnEOTSurprisalFormula}{-\log P(\token{EOT} \mid \word{1}[2], \ldots, \word{N}[2], \word{1}[1], \ldots, \word{K}[1])}




% % general math
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \newcommand{\expectedvalue}{\mathbb{E}_{\policy}}
% \newcommand{\true}{\mathit{true}}
% \newcommand{\false}{\mathit{false}}


% % STRIPS
% \newcommand{\task}{T}
% \newcommand{\fluents}{F}
% \newcommand{\literals}{L}
% \newcommand{\operators}{O}
% \newcommand{\initfluentstate}{\sigma_0}
% \newcommand{\goal}{\partialfluentstate_g}
% \newcommand{\subgoal}{\partialfluentstate_{sg}}
% \newcommand{\subgoals}{\Sigma_{sg}}
% \newcommand{\fluent}{f}
% \newcommand{\literal}{l}
% \newcommand{\fluentstate}{\sigma}
% \newcommand{\partialfluentstate}{\tilde{\sigma}}
% \newcommand{\operator}{o}
% \newcommand{\partialoperator}{\tilde{o}}
% \newcommand{\plan}{\pi_T}
% \newcommand{\precon}{pre}
% \newcommand{\effect}{\mathit{eff}}
% \newcommand{\addeffect}{\mathit{eff}^+}
% \newcommand{\deleffect}{\mathit{eff}^-}
% \newcommand{\static}{static}
% \newcommand{\unknown}{unknown}
% \newcommand{\allliterals}{\mathcal{L}(\fluents)}
% \newcommand{\fluenttransition}{\delta(\partialfluentstate, \operator)}
% \newcommand{\applyoperator}[2]{\delta(#1, #2)}
% \newcommand{\regressoperator}[2]{\delta^{-1}(#1, #2)}
% \newcommand{\applyfunction}{\delta}
% \newcommand{\regressfunction}{\delta^{-1}}

% \newcommand{\reachable}{\Delta_{\partialfluentstate}}
% \newcommand{\planstates}{\Sigma_{reach}}
% \newcommand{\unplanstates}{\Sigma_{dead}}
% \newcommand{\restrict}{restrict}

% \newcommand{\planfluentstates}{\Sigma_{plan}}
% \newcommand{\reachfluentstates}{\Sigma_{reach}}
% \newcommand{\deadfluentstates}{\Sigma_{dead}}


% % MDP
% \newcommand{\mdp}{M}
% \newcommand{\mstates}{S}
% \newcommand{\actions}{A}
% \newcommand{\rewardfunction}{r}
% \newcommand{\transitionprob}{p}
% \newcommand{\discountfactor}{\gamma}
% \newcommand{\mstate}{s}
% \newcommand{\initialstatedist}{\iota}
% \newcommand{\initialstate}{\mstate_0}
% \newcommand{\action}{a}
% \newcommand{\valuefunction}{v_{\policy(\mstate)}}
% \newcommand{\policy}{\pi_M}
% \newcommand{\policies}{\Pi_M}
% \newcommand{\achievable}{\Omega_{\mstate}}
% \newcommand{\qfunction}{q(\mstate, \action)}
% \newcommand{\qfunctionopt}{q^*(\mstate, \action)}
% \newcommand{\qfunctionnext}{q(\newstate, \newaction)}
% \newcommand{\learningrate}{\alpha}
% \newcommand{\experience}{(\mstate, \action, \rewardfunction, \newstate}
% \newcommand{\newstate}{\mstate^\prime}
% \newcommand{\newaction}{\action^\prime}
% \newcommand{\onlinelearner}{\ell_{expl}}
% \newcommand{\offlinelearner}{\ell}
% \newcommand{\learners}{L}
% \newcommand{\threshold}{\tau}

% \newcommand{\exec}{x}
% \newcommand{\execinit}{I_x}
% \newcommand{\execpolicy}{\pi_x}
% \newcommand{\execterm}{\beta_x}
% \newcommand{\execs}{X}

% \newcommand{\execstar}{x^\star}
% \newcommand{\execinitstar}{I_{x^\star}}
% \newcommand{\execpolicystar}{\pi_{x^\star}}
% \newcommand{\exectermstar}{\beta_{x^\star}}

% \newcommand{\execinitfor}[1]{I_{#1}}
% \newcommand{\execpolicyfor}[1]{\pi_{#1}}
% \newcommand{\exectermfor}[1]{\beta_{#1}}

% \newcommand{\explorationpolicy}{\pi_{expl}}

% % Interated stuff
% \newcommand{\symdp}{\mathcal{T}}
% \newcommand{\detector}{d}
% \newcommand{\executor}{e}
% \newcommand{\macgyver}{\tilde{\symdp}}

% % Learning preconditions
% \newcommand{\applicablefluentstates}{\Sigma_{app}}
% \newcommand{\beenfluentstates}{\Sigma_{been}}
% \newcommand{\abovethresholdfluentstates}{\Sigma_{>\tau}}
% \newcommand{\node}{node}
% \newcommand{\common}{\partialfluentstate_{common}}
% \newcommand{\anotherfluentstate}{\fluentstate^\prime}



% % Target MDP
% \newcommand{\targetmdp}{M_T}
% \newcommand{\bsn}{\subgoal}

% % Agent level
% \newcommand{\lflag}{\mathit{impasse}}

% % Algorithm stuff
% \newcommand{\solve}{\textbf{\textsc{solve}}}
% \newcommand{\learn}{\textbf{\textsc{learn}}}
% \newcommand{\genprecon}{\textbf{\textsc{gen-precon}}}
% \newcommand{\execute}{\textsc{execute}}
% \newcommand{\env}{\mbox{ENV}}
% \newcommand{\owfs}{\textsc{owfs}}
% \newcommand{\owbs}{\textsc{owbs}}

