
Transformer-based Large Language Models (LLMs) have recently increased in popularity, in part due their impressive performance on a number of language tasks. While LLMs can produce human-like writing, the extent to which these models can learn to predict \emph{spoken} language in natural interaction remains unclear. This is a non-trivial question, as spoken and written language differ in syntax, pragmatics, and norms that interlocutors follow. Previous work suggests that while LLMs may develop an understanding of linguistic rules based on statistical regularities, they fail to acquire the knowledge required for language use. This implies that LLMs may not learn the normative structure underlying interactive spoken language, but may instead only model superficial regularities in speech. In this paper, we aim to evaluate LLMs as models of spoken dialogue. Specifically, we investigate whether LLMs can learn that the \emph{identity} of a speaker in spoken dialogue influences what is likely to be said. To answer this question, we first fine-tuned two variants of a specific LLM (GPT-2) on transcripts of natural spoken dialogue in English. Then we used these models to compute surprisal values for two-turn sequences with the same first-turn but different second-turn speakers. While all fine-tuned models used speaker identity to predict turns, they sometimes hallucinated speaker transitions to make certain stimuli more plausible. Our findings suggest that although LLMs may learn to generate text conforming to normative linguistic structure, they do not (yet) faithfully replicate human behavior in natural conversation.
\keywords{Generative pre-trained Transformers; Natural Language Processing; Language in Interaction}
